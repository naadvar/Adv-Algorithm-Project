from pyspark.sql import functions as F
from pyspark.sql.types import StructType, StructField, StringType, ArrayType, MapType

# Define a recursive function to clean up empty values within nested structures and track removed keys
def clean_nested_structure(data, parent_key=None):
    removed_keys = []
    
    if isinstance(data, dict):
        cleaned_data = {}
        for k, v in data.items():
            if v in [None, "", " "]:
                removed_keys.append(k if parent_key is None else f"{parent_key}.{k}")  # Track the key if it's removed
            else:
                cleaned_value, nested_removed_keys = clean_nested_structure(v, parent_key=k)
                cleaned_data[k] = cleaned_value
                removed_keys.extend(nested_removed_keys)
        return cleaned_data, removed_keys
    
    elif isinstance(data, list):
        cleaned_data = []
        for i, item in enumerate(data):
            if item in [None, "", " "]:
                # Track the index if an item is removed in the list context
                removed_keys.append(f"{parent_key}[{i}]") if parent_key else None
            else:
                cleaned_value, nested_removed_keys = clean_nested_structure(item, parent_key=f"{parent_key}[{i}]" if parent_key else None)
                cleaned_data.append(cleaned_value)
                removed_keys.extend(nested_removed_keys)
        return cleaned_data, removed_keys

    elif isinstance(data, Row):
        # Convert Row to dictionary, clean, and keep track of removed keys
        cleaned_data = {}
        for field, value in data.asDict().items():
            if value in [None, "", " "]:
                removed_keys.append(field if parent_key is None else f"{parent_key}.{field}")  # Track the field if it's removed
            else:
                cleaned_value, nested_removed_keys = clean_nested_structure(value, parent_key=field)
                cleaned_data[field] = cleaned_value
                removed_keys.extend(nested_removed_keys)
        return cleaned_data, removed_keys

    else:
        # For primitive types (like strings, numbers, etc.), treat them directly
        # If the value is invalid, add it to removed keys
        if data in [None, "", " "]:
            removed_keys.append(parent_key)  # Track its location using the parent key
            return None, removed_keys  # Return None as a cleaned value since it's "removed"
        return data, removed_keys  # Otherwise, return it as-is with no removed keys

# Define a helper UDF function to clean the nested structure and return both cleaned data and removed keys
def udf_clean_nested_structure(data):
    cleaned_data, removed_keys = clean_nested_structure(data)
    return (cleaned_data, removed_keys)

# Define the schema for the returned struct from the UDF
schema = StructType([
    StructField("cleaned_data", MapType(StringType(), StringType())),  # Modify this based on your structure's data types
    StructField("removed_keys", ArrayType(StringType()))
])

# Register the UDF with the defined schema
clean_nested_structure_udf = F.udf(udf_clean_nested_structure, schema)

# Apply the UDF to the entire experianCreditBureau structure
exp_1 = exp_1.withColumn(
    'cleaned_experianCreditBureau',
    clean_nested_structure_udf(F.col('experianResponse.experianCreditBureau'))
)

# Update experianCreditBureau with cleaned data and extract removed keys as a separate column
exp_1 = exp_1.withColumn(
    "experianResponse.experianCreditBureau", 
    F.col("cleaned_experianCreditBureau.cleaned_data")
).withColumn(
    "removed_keys", 
    F.col("cleaned_experianCreditBureau.removed_keys")
).drop("cleaned_experianCreditBureau")

# Show the modified DataFrame with both cleaned experianCreditBureau and removed keys
exp_1.select('experianResponse.experianCreditBureau', 'removed_keys').show(truncate=False)

from pyspark.sql.functions import col, from_json, to_json, expr
from pyspark.sql.types import StringType, MapType

# Parse the 'experianResponse' JSON column into a Spark DataFrame column with MapType
json_schema = MapType(StringType(), StringType())  # Assuming JSON has a key-value structure

parsed_df = exp_2.withColumn("parsed_json", from_json(col("experianResponse"), json_schema))

# Filter out null, empty strings, and empty lists/dictionaries in JSON using Spark SQL
cleaned_df = parsed_df.withColumn(
    "cleaned_json",
    expr("""
        to_json(transform_keys(parsed_json, (k, v) -> 
            CASE 
                WHEN v IS NULL OR v = '' OR v = '{}' OR v = '[]' THEN null
                ELSE v
            END
        ))
    """)
)

# Optionally drop the original JSON column and intermediate parsed column
final_df = cleaned_df.drop("experianResponse", "parsed_json")

# Display the cleaned JSON column
final_df.select("cleaned_json").show(truncate=False)


from pyspark.sql.functions import col, struct, when

# Function to recursively clean a struct
def clean_nested_json(schema, col_name):
    fields = []
    for field in schema.fields:
        field_name = field.name
        field_type = field.dataType
        if isinstance(field_type, StructType):
            # Recursively handle nested structs
            fields.append(
                (col(field_name).isNotNull(), clean_nested_json(field_type, field_name))
            )
        else:
            fields.append(
                when(
                    (col(field_name).isNotNull()) & (col(field_name) != "") & (col(field_name) != "{}") & (col(field_name) != "[]"),
                    col(field_name),
                )
            )
    return struct(*fields)

# Apply the cleaning function
cleaned_df = parsed_df.withColumn("cleaned_json", clean_nested_json(parsed_df.schema, "parsed_json"))
