from pyspark.sql import SparkSession
from pyspark.sql.functions import col, lit, when
from pyspark.sql.types import StructType, StructField, ArrayType, MapType

# Initialize Spark session
spark = SparkSession.builder.appName("FindZeroValues").getOrCreate()

def get_column_paths(schema, parent=""):
    """
    Recursively find all paths to scalar fields in a nested DataFrame schema.
    """
    paths = []
    for field in schema.fields:
        field_name = f"{parent}.{field.name}" if parent else field.name
        if isinstance(field.dataType, StructType):
            # Recurse into structs
            paths.extend(get_column_paths(field.dataType, field_name))
        elif isinstance(field.dataType, ArrayType) and isinstance(field.dataType.elementType, StructType):
            # Handle arrays of structs by appending array notation
            paths.extend(get_column_paths(field.dataType.elementType, f"{field_name}"))
        elif isinstance(field.dataType, (ArrayType, MapType)):
            # Skip complex nested types not directly filterable (e.g., arrays of arrays or maps)
            continue
        else:
            # For scalar fields, just add the path
            paths.append(field_name)
    return paths

# Load your DataFrame
# df = spark.read.json("path_to_your_json_file.json")

# Sample DataFrame schema - assuming 'df' is your DataFrame
column_paths = get_column_paths(df.schema)

# Now we filter columns where values are 0 and collect those paths
zero_value_columns = []
for path in column_paths:
    zero_condition = when(col(path) == 0, lit(path)).otherwise(None)
    filtered_df = df.withColumn(path + "_zero", zero_condition)

# Collect the paths of columns with zero values
zero_value_columns = [
    path for path in column_paths
    if df.select(col(path)).filter(col(path) == 0).count() > 0
]

# Display paths with zero values
print("Paths with zero values:", zero_value_columns)
