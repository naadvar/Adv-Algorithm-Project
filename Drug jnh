from pyspark.sql import SparkSession
from pyspark.sql.functions import col, lit, when, explode, array_contains
from pyspark.sql.types import StructType, StructField, ArrayType, MapType

# Initialize Spark session
spark = SparkSession.builder.appName("FindZeroValues").getOrCreate()

def get_column_paths(schema, parent=""):
    """
    Recursively find all paths to scalar fields in a nested DataFrame schema.
    """
    paths = []
    for field in schema.fields:
        field_name = f"{parent}.{field.name}" if parent else field.name
        if isinstance(field.dataType, StructType):
            # Recurse into structs
            paths.extend(get_column_paths(field.dataType, field_name))
        elif isinstance(field.dataType, ArrayType):
            if isinstance(field.dataType.elementType, StructType):
                # Recurse into structs within arrays
                paths.extend(get_column_paths(field.dataType.elementType, f"{field_name}"))
            else:
                # Handle arrays of scalars by noting it in the paths list
                paths.append((field_name, 'array'))
        else:
            # For scalar fields, just add the path
            paths.append((field_name, 'scalar'))
    return paths

# Load your DataFrame
# df = spark.read.json("path_to_your_json_file.json")

# Sample DataFrame schema - assuming 'df' is your DataFrame
column_paths = get_column_paths(df.schema)

# Now we filter columns where values are 0 and collect those paths
zero_value_columns = []
for path, path_type in column_paths:
    try:
        # Split the path and use getField for nested fields
        fields = path.split(".")
        expr = col(fields[0])
        for field in fields[1:]:
            expr = expr.getField(field)

        # Check for zero value, handling arrays separately
        if path_type == 'array':
            # For arrays, check if any element is zero
            zero_condition = when(array_contains(expr, lit(0)), lit(path)).otherwise(None)
        else:
            # For scalars, direct comparison to zero
            zero_condition = when(expr == 0, lit(path)).otherwise(None)

        # Create a new column to check for zero values
        filtered_df = df.withColumn(path.replace(".", "_") + "_zero", zero_condition)
        
        # Collect columns with zero values
        if filtered_df.filter(col(path.replace(".", "_") + "_zero").isNotNull()).count() > 0:
            zero_value_columns.append(path)
            
    except Exception as e:
        print(f"Error processing path {path}: {e}")

# Display paths with zero values
print("Paths with zero values:", zero_value_columns)
