from pyspark.sql import functions as F
from pyspark.sql.types import StructType, StructField, StringType, LongType, ArrayType, MapType

# Define a debug function to print the contents of bestAttributes
def debug_best_attributes(best_attributes):
    if best_attributes is None:
        return "None"
    if isinstance(best_attributes, dict):
        return str(best_attributes)
    elif isinstance(best_attributes, list):
        return str([str(item) for item in best_attributes])
    else:
        return "Unexpected type: " + str(type(best_attributes))

# Register the debug function as a UDF
debug_best_attributes_udf = F.udf(debug_best_attributes, StringType())

# Apply the debug function to bestAttributes field to inspect its content
debug_df = exp_1.withColumn(
    'bestAttributes_debug', 
    debug_best_attributes_udf(F.col('experianResponse.experianCreditBureau.products.customSolution.bestAttributes'))
)

# Show the debug output
debug_df.select('bestAttributes_debug').show(truncate=False)
