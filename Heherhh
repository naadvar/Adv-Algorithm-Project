import re
from pyspark.sql import functions as F
from pyspark.sql.types import StructType, StructField, StringType, ArrayType, MapType, Row

# Define a recursive function to clean up empty values within nested structures and track removed keys
def clean_nested_structure(data, parent_key=None):
    removed_keys = []
    whitespace_pattern = re.compile(r'^\s+$')  # Regex pattern to match strings with only whitespace

    if isinstance(data, dict):
        cleaned_data = {}
        for k, v in data.items():
            if v is None or (isinstance(v, str) and whitespace_pattern.match(v)):
                removed_keys.append(k if parent_key is None else f"{parent_key}.{k}")  # Track the key if it's removed
            else:
                cleaned_value, nested_removed_keys = clean_nested_structure(v, parent_key=k)
                cleaned_data[k] = cleaned_value
                removed_keys.extend(nested_removed_keys)
        return cleaned_data, removed_keys
    
    elif isinstance(data, list):
        cleaned_data = []
        for i, item in enumerate(data):
            if item is None or (isinstance(item, str) and whitespace_pattern.match(item)):
                # Track the index if an item is removed in the list context
                removed_keys.append(f"{parent_key}[{i}]") if parent_key else None
            else:
                cleaned_value, nested_removed_keys = clean_nested_structure(item, parent_key=f"{parent_key}[{i}]" if parent_key else None)
                cleaned_data.append(cleaned_value)
                removed_keys.extend(nested_removed_keys)
        return cleaned_data, removed_keys

    elif isinstance(data, Row):
        # Convert Row to dictionary, clean, and keep track of removed keys
        cleaned_data = {}
        for field, value in data.asDict().items():
            if value is None or (isinstance(value, str) and whitespace_pattern.match(value)):
                removed_keys.append(field if parent_key is None else f"{parent_key}.{field}")  # Track the field if it's removed
            else:
                cleaned_value, nested_removed_keys = clean_nested_structure(value, parent_key=field)
                cleaned_data[field] = cleaned_value
                removed_keys.extend(nested_removed_keys)
        # Convert cleaned_data back to Row to maintain original JSON structure
        return Row(**cleaned_data), removed_keys

    else:
        # For primitive types (like strings, numbers, etc.), treat them directly
        # If the value is invalid (None or whitespace-only string), add it to removed keys
        if data is None or (isinstance(data, str) and whitespace_pattern.match(data)):
            removed_keys.append(parent_key)  # Track its location using the parent key
            return None, removed_keys  # Return None as a cleaned value since it's "removed"
        return data, removed_keys  # Otherwise, return it as-is with no removed keys

# Define a helper UDF function to clean the nested structure and return both cleaned data and removed keys
def udf_clean_nested_structure(data):
    cleaned_data, removed_keys = clean_nested_structure(data)
    return (cleaned_data, removed_keys)

# Define a general schema for the UDF, treating cleaned_data as a MapType for simplicity.
# Alternatively, you could specify the exact structure if known.
schema = StructType([
    StructField("cleaned_data", MapType(StringType(), StringType())),  # Use MapType for generalized nested structure
    StructField("removed_keys", ArrayType(StringType()))
])

# Register the UDF with the defined schema
clean_nested_structure_udf = F.udf(udf_clean_nested_structure, schema)

# Apply the UDF to the entire experianCreditBureau structure
exp_1 = exp_1.withColumn(
    'cleaned_experianCreditBureau',
    clean_nested_structure_udf(F.col('experianResponse.experianCreditBureau'))
)

# Update experianCreditBureau with cleaned data and extract removed keys as a separate column
exp_1 = exp_1.withColumn(
    "experianResponse.experianCreditBureau", 
    F.col("cleaned_experianCreditBureau.cleaned_data")
).withColumn(
    "removed_keys", 
    F.col("cleaned_experianCreditBureau.removed_keys")
).drop("cleaned_experianCreditBureau")

# Show the modified DataFrame with both cleaned experianCreditBureau and removed keys
exp_1.select('experianResponse.experianCreditBureau', 'removed_keys').show(truncate=False)
