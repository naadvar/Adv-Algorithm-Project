from pyspark.sql import SparkSession
from pyspark.sql.functions import col, lit, when
from pyspark.sql.types import StructType, StructField, ArrayType, MapType

# Initialize Spark session
spark = SparkSession.builder.appName("FindZeroValuesInBestAttributes").getOrCreate()

def get_best_attributes_paths(schema, parent=""):
    """
    Recursively find all paths in 'bestAttributes' where we need to check for zero values.
    """
    paths = []
    for field in schema.fields:
        field_name = f"{parent}.{field.name}" if parent else field.name
        if field_name.endswith("bestAttributes") and isinstance(field.dataType, StructType):
            # Only collect fields under 'bestAttributes'
            paths.extend(get_column_paths(field.dataType, field_name))
        elif isinstance(field.dataType, StructType):
            # Recurse into structs
            paths.extend(get_best_attributes_paths(field.dataType, field_name))
    return paths

def get_column_paths(schema, parent=""):
    """
    Recursively find all scalar fields in a nested StructType.
    """
    paths = []
    for field in schema.fields:
        field_name = f"{parent}.{field.name}" if parent else field.name
        if isinstance(field.dataType, StructType):
            # Recurse into structs
            paths.extend(get_column_paths(field.dataType, field_name))
        elif isinstance(field.dataType, ArrayType) and isinstance(field.dataType.elementType, StructType):
            # Handle arrays of structs by appending array notation
            paths.extend(get_column_paths(field.dataType.elementType, f"{field_name}"))
        elif isinstance(field.dataType, (ArrayType, MapType)):
            # Skip complex nested types not directly filterable (e.g., arrays of arrays or maps)
            continue
        else:
            # For scalar fields, just add the path
            paths.append(field_name)
    return paths

# Load your DataFrame
# df = spark.read.json("path_to_your_json_file.json")

# Get only paths under bestAttributes for zero-value checking
best_attributes_paths = get_best_attributes_paths(df.schema)

# Now we filter columns where values are 0 and collect those paths
zero_value_columns = []
for path in best_attributes_paths:
    try:
        # Split the path and use getField for nested fields
        fields = path.split(".")
        expr = col(fields[0])
        for field in fields[1:]:
            expr = expr.getField(field)

        # Check for zero value
        zero_condition = when(expr == 0, lit(path)).otherwise(None)

        # Create a new column to check for zero values
        filtered_df = df.withColumn(path.replace(".", "_") + "_zero", zero_condition)
        
        # Collect columns with zero values specifically under bestAttributes
        if filtered_df.filter(col(path.replace(".", "_") + "_zero").isNotNull()).count() > 0:
            zero_value_columns.append(path)
            
    except Exception as e:
        print(f"Error processing path {path}: {e}")

# Display paths with zero values in bestAttributes
print("Paths with zero values in bestAttributes:", zero_value_columns)
from pyspark.sql import functions as F
from pyspark.sql.types import StructType, StructField, StringType, LongType, ArrayType, MapType

# Define a function to filter out Row objects with value 0 and collect removed keys
def filter_best_attributes(best_attributes):
    filtered_attributes = {}
    removed_keys = []
    
    # Check if best_attributes is a list of Rows
    if isinstance(best_attributes, list):
        for item in best_attributes:
            if hasattr(item, "__fields__"):
                # Assuming each Row has one field with key and value
                for key in item.__fields__:
                    value = getattr(item, key)
                    if value != 0:
                        filtered_attributes[key] = value
                    else:
                        removed_keys.append(key)
    
    # Return the filtered attributes and removed keys as a tuple
    return (filtered_attributes, removed_keys)

# Register the function as a UDF with the appropriate return types
filter_best_attributes_udf = F.udf(filter_best_attributes, 
                                   StructType([
                                       StructField("filtered_attributes", MapType(StringType(), LongType())),
                                       StructField("removed_keys", ArrayType(StringType()))
                                   ]))

# Apply the function to the bestAttributes field
filtered_df = exp_1.withColumn(
    'result',
    filter_best_attributes_udf(F.col('experianResponse.experianCreditBureau.products.customSolution.bestAttributes'))
)

# Extract filtered attributes and removed keys from the result column
filtered_df = filtered_df.withColumn("filtered_bestAttributes", F.col("result.filtered_attributes")) \
                         .withColumn("removed_keys", F.col("result.removed_keys")) \
                         .drop("result")

# Show the resulting DataFrame with filtered attributes and removed keys
filtered_df.select('filtered_bestAttributes', 'removed_keys').show(truncate=False)
