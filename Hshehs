from pyspark.sql import functions as F

# Define a function to filter out keys with value 0 and collect removed keys
def filter_best_attributes(row):
    # Extract the 'bestAttributes' dictionary from the row
    best_attributes = row['experianResponse']['experianCreditBureau']['products']['customSolution']['bestAttributes']
    
    # Filter out keys with a value of 0 and keep track of removed keys
    filtered_attributes = {}
    removed_keys = []
    
    for key, value in best_attributes.items():
        if value != 0:
            filtered_attributes[key] = value
        else:
            removed_keys.append(key)
    
    # Return the filtered attributes and removed keys
    return (filtered_attributes, removed_keys)

# Apply the function to each row and add new columns for filtered attributes and removed keys
filtered_df = exp_1.withColumn(
    'filtered_bestAttributes', 
    F.udf(lambda row: filter_best_attributes(row)[0])('experianResponse.experianCreditBureau.products.customSolution.bestAttributes')
).withColumn(
    'removed_keys',
    F.udf(lambda row: filter_best_attributes(row)[1])('experianResponse.experianCreditBureau.products.customSolution.bestAttributes')
)

# Show the resulting DataFrame with filtered attributes and removed keys
filtered_df.select('filtered_bestAttributes', 'removed_keys').show(truncate=False)
