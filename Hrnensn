import pandas as pd

# Sample data as a pandas DataFrame
data = {
    'Job Level': [[3.0, 2.0, 4.0, 1.0, 3.0],
                  [float('nan'), 1.0, 3.0, 2.0, 4.0],
                  [2.0, 3.0, 1.0, 4.0, 3.0]],
    'Organization': [['Marketing', 'Finance', 'HR', 'IT', 'Marketing'],
                     ['HR', 'Marketing', 'Finance', 'IT', 'HR'],
                     ['Finance', 'HR', 'Marketing', 'IT', 'Finance']],
    'Employee ID': [['EMP001', 'EMP002', 'EMP003', 'EMP004', 'EMP005'],
                    ['EMP006', 'EMP007', 'EMP008', 'EMP009', 'EMP010'],
                    ['EMP011', 'EMP012', 'EMP013', 'EMP014', 'EMP015']],
    'Meeting ID': [1, 1, 2, 2, 3],
    'Meeting Date': ['2024-04-28', '2024-04-28', '2024-04-29', '2024-04-29', '2024-04-30']
}

df = pd.DataFrame(data)

# Function to count the number of times each person was the most senior member in his organization for each meeting ID and date
def count_most_senior_in_organization(df):
    senior_count_by_meeting = {}
    senior_count_overall = {}
    total_meetings_attended = {}
    max_levels = {}
    
    for idx, row in df.iterrows():
        job_levels = row['Job Level']
        organizations = row['Organization']
        employee_ids = row['Employee ID']
        meeting_id = row['Meeting ID']
        meeting_date = row['Meeting Date']
        
        for level, org, emp in zip(job_levels, organizations, employee_ids):
            total_meetings_attended[emp] = total_meetings_attended.get(emp, 0) + 1
            
            if (meeting_id, meeting_date) not in max_levels:
                max_levels[(meeting_id, meeting_date)] = {}
                senior_count_by_meeting[(meeting_id, meeting_date)] = defaultdict(int)
                
            if org not in max_levels[(meeting_id, meeting_date)] or level > max_levels[(meeting_id, meeting_date)][org]:
                max_levels[(meeting_id, meeting_date)][org] = level
                senior_count_by_meeting[(meeting_id, meeting_date)][emp] = 1
                senior_count_overall[emp] = senior_count_overall.get(emp, 0) + 1
            elif level == max_levels[(meeting_id, meeting_date)][org]:
                senior_count_by_meeting[(meeting_id, meeting_date)][emp] += 1
                senior_count_overall[emp] = senior_count_overall.get(emp, 0) + 1
                
    return senior_count_by_meeting, senior_count_overall, total_meetings_attended

# Count the number of times each person was the most senior member in his organization for each meeting ID and date, overall, and total meetings attended
senior_count_by_meeting, senior_count_overall, total_meetings_attended = count_most_senior_in_organization(df)

# Print the counts for each meeting ID and date
print("Senior count by meeting ID and date:")
for (meeting_id, meeting_date), count in senior_count_by_meeting.items():
    print(f"Meeting ID: {meeting_id}, Date: {meeting_date}, Senior count: {count}")

# Print the overall counts
print("\nOverall senior count:")
for emp, count in senior_count_overall.items():
    print(f"Employee ID: {emp}, Overall senior count: {count}")

# Print the total meetings attended by each person
print("\nTotal meetings attended:")
for emp, count in total_meetings_attended.items():
    print(f"Employee ID: {emp}, Total meetings attended: {count}")

# Calculate and print the ratio of being the most senior member in the organization to the total number of meetings attended
print("\nRatio of being the most senior member in the organization to total meetings attended:")
for emp in senior_count_overall.keys():
    ratio = senior_count_overall[emp] / total_meetings_attended[emp]
    print(f"Employee ID: {emp}, Ratio: {ratio:.2f}")
import pandas as pd

# Load the data into a DataFrame
df = pd.read_csv('employee_performance.csv')

# Convert end_of_month_date to datetime
df['end_of_month_date'] = pd.to_datetime(df['end_of_month_date'])

# Sort the DataFrame by employee_id and end_of_month_date
df = df.sort_values(by=['employee_id', 'end_of_month_date'])

# Create a new column to indicate if the performance was 'Below Strong'
df['below_strong'] = df['performance_category'].apply(lambda x: 1 if x == 'Below Strong' else 0)

# Define a function to calculate the rolling count of below strong ratings in the past N days
def rolling_below_strong_count(df, days):
    return df.groupby('employee_id').apply(
        lambda x: x.set_index('end_of_month_date')['below_strong']
                   .rolling(f'{days}D').sum().reset_index(drop=True)
    ).reset_index(drop=True)

# Calculate the rolling count for the past 6 months (approx. 180 days) and 12 months (approx. 365 days)
df['below_strong_last_6_months'] = rolling_below_strong_count(df, 180)
df['below_strong_last_12_months'] = rolling_below_strong_count(df, 365)

# Function to calculate the months since first 'Below Strong' rating
def months_since_first_below_strong(df):
    first_below_strong_dates = df[df['below_strong'] == 1].groupby('employee_id')['end_of_month_date'].min()
    df = df.merge(first_below_strong_dates.rename('first_below_strong_date'), on='employee_id', how='left')
    df['months_since_first_below_strong'] = df.apply(
        lambda row: (row['end_of_month_date'] - row['first_below_strong_date']).days // 30 
                    if pd.notnull(row['first_below_strong_date']) else None,
        axis=1
    )
    return df

# Apply the function to calculate months since first 'Below Strong' rating
df = months_since_first_below_strong(df)

# Drop the temporary columns used for calculations
df.drop(columns=['below_strong', 'first_below_strong_date'], inplace=True)

# Display the resulting DataFrame
print(df)


def simulate_employee_attrition(employee_id, attrition_risk, num_months=6):
    """Simulate whether an employee attrits each month."""
    attrition_status = []
    for month in range(num_months):
        if np.random.rand() < attrition_risk:
            attrition_status.append(True)
        else:
            attrition_status.append(False)
    return attrition_status

# Apply simulation to each employee
df['Monthly_Attrition'] = df.apply(lambda row: simulate_employee_attrition(row['Employee_ID'], row['Attrition_Risk']), axis=1)

# View results
print(df[['Employee_ID', 'Attrition_Risk', 'Monthly_Attrition']].head())


def simulate_group_attrition(df, num_simulations=1000):
    num_employees = len(df)
    num_months = 6
    results = []

    for _ in range(num_simulations):
        total_attritions = 0
        for index, row in df.iterrows():
            attrition_risk = row['Attrition_Risk']
            attrition_occurred = False
            for month in range(num_months):
                if np.random.rand() < attrition_risk:
                    attrition_occurred = True
                    break
            if attrition_occurred:
                total_attritions += 1
        results.append(total_attritions)

    return results

# Run group attrition simulation
results = simulate_group_attrition(df)

# Analyze results
import matplotlib.pyplot as plt

plt.hist(results, bins=30, edgecolor='k', alpha=0.7)
plt.xlabel('Number of Employees Attriting At Least Once Over 6 Months')
plt.ylabel('Frequency')
plt.title('Monte Carlo Simulation of Employee Attrition (Group Level)')
plt.show()

print(f"Mean attritions over 6 months: {np.mean(results):.2f}")
print(f"Standard deviation of attritions: {np.std(results):.2f}")
print(f"95% confidence interval: {np.percentile(results, [2.5, 97.5])}")


import numpy as np
import matplotlib.pyplot as plt

def simulate_attrition(attrition_prob, num_employees, num_months=6):
    """Simulate attrition for a group of employees over a specified number of months."""
    # Create an array to store the attrition status for each employee for each month
    attrition_status = np.zeros((num_employees, num_months))
    
    for employee in range(num_employees):
        for month in range(num_months):
            # Determine if the employee attrits this month
            if np.random.rand() < attrition_prob:
                attrition_status[employee, month] = 1
                break  # Once an employee attrits, stop checking further months

    return attrition_status

def monte_carlo_simulation(attrition_prob, num_employees, num_simulations=1000, num_months=6):
    """Run Monte Carlo simulations to model employee attrition."""
    simulation_results = []

    for _ in range(num_simulations):
        attrition_status = simulate_attrition(attrition_prob, num_employees, num_months)
        total_attritions = np.sum(np.any(attrition_status, axis=1))
        simulation_results.append(total_attritions)

    return simulation_results

# Parameters
attrition_prob = 0.05  # Example attrition probability
num_employees = 100  # Number of employees in each simulation
num_simulations = 1000  # Number of simulations to run
num_months = 6  # Number of months to simulate

# Run Monte Carlo simulation
results = monte_carlo_simulation(attrition_prob, num_employees, num_simulations, num_months)

# Analyze and plot the results
plt.hist(results, bins=30, edgecolor='k', alpha=0.7)
plt.xlabel('Number of Employees Attriting At Least Once Over 6 Months')
plt.ylabel('Frequency')
plt.title('Monte Carlo Simulation of Employee Attrition')
plt.show()

# Calculate summary statistics
mean_attrition = np.mean(results)
std_attrition = np.std(results)
confidence_interval = np.percentile(results, [2.5, 97.5])

print(f"Mean number of employees attriting at least once: {mean_attrition:.2f}")
print(f"Standard deviation of attritions: {std_attrition:.2f}")
print(f"95% confidence interval: {confidence


_interval}")



import dask.dataframe as dd
import numpy as np

# Assuming 'data' is initially a Pandas DataFrame, first convert it to a Dask DataFrame
data = dd.from_pandas(data, npartitions=4)

# Perform renaming and type conversion
data = data.rename(columns={"snap_dt": "mstr_dt"})
data["mstr_dt"] = dd.to_datetime(data["mstr_dt"]).astype('datetime64[ns]')
data["year"] = data["mstr_dt"].dt.year
data["month"] = data["mstr_dt"].dt.month
data["performance"] = data["performance"].fillna(0)
data["promo_int"] = data["promo_int"].fillna(0)

# Remove duplicates and reset index
data = data.drop_duplicates(subset=["mstr_dt", "emp_id"]).reset_index(drop=True)

# Add sinusoidal transformation of the month
data["data_anch"] = np.sin(7 * data["performance"])

# Sort values
data = data.sort_values(["emp_id", "mstr_dt"])

# Create performance dictionary
perf_dict = {1: "Below Strong", 2: "Strong", 3: "Above Strong"}

# Map performance values
data["performance_values"] = data["performance"].map(perf_dict)

# Calculate differences
data["performance_values_diff"] = data.groupby("emp_id")["performance_values"].diff(6)
data["mom_performance_change"] = data.groupby("emp_id")["performance_values"].diff(6).fillna(0)

# Fill NaNs with 0 for differences
data["mom_performance_change_inc"] = np.where(data["mom_performance_change"] > 0, 1, 0)
data["mom_performance_change_dec"] = np.where(data["mom_performance_change"] < 0, 1, 0)

# Calculate performance changes over rolling window
data["perf_increase_count_18_months"] = (
    data.sort_values(["emp_id", "mstr_dt"])
    .groupby("emp_id")["mom_performance_change_inc"]
    .rolling(12, min_periods=1)
    .sum()
    .reset_index(level=0, drop=True)
)

data["perf_decrease_count_18_months"] = (
    data.sort_values(["emp_id", "mstr_dt"])
    .groupby("emp_id")["mom_performance_change_dec"]
    .rolling(12, min_periods=1)
    .sum()
    .reset_index(level=0, drop=True)
)

# Convert performance categories to integers and calculate cumulative sums
data["performance_category_strong_int"] = (data.performance == "Strong").astype(int)
data["performance_category_strong_cumsum"] = (
    data.groupby("emp_id")["performance_category_strong_int"]
    .cumsum()
    .fillna(0)
    .reset_index(drop=True)
)

data["performance_category_below_strong_int"] = (data.performance == "Below Strong").astype(int)
data["performance_category_below_strong_cumsum"] = (
    data.groupby("emp_id")["performance_category_below_strong_int"]
    .cumsum()
    .fillna(0)
    .reset_index(drop=True)
)

# Remember to compute the final results if needed
data = data.compute()


import dask.dataframe as dd
import numpy as np
import pandas as pd

# Example DataFrame creation for illustration purposes
data = pd.DataFrame({
    'snap_dt': pd.date_range('2023-01-01', periods=100, freq='M'),
    'emp_id': np.random.randint(1, 10, size=100),
    'performance': np.random.randint(1, 4, size=100),
    'promo_int': np.random.randint(0, 2, size=100)
})

# Convert the Pandas DataFrame to a Dask DataFrame
data = dd.from_pandas(data, npartitions=4)

# Perform renaming and type conversion
data = data.rename(columns={"snap_dt": "mstr_dt"})
data["mstr_dt"] = dd.to_datetime(data["mstr_dt"]).astype('datetime64[ns]')
data["year"] = data["mstr_dt"].dt.year
data["month"] = data["mstr_dt"].dt.month
data["performance"] = data["performance"].fillna(0)
data["promo_int"] = data["promo_int"].fillna(0)

# Remove duplicates and reset index
data = data.drop_duplicates(subset=["mstr_dt", "emp_id"]).reset_index(drop=True)

# Add sinusoidal transformation of the month
data["data_anch"] = np.sin(7 * data["performance"])

# Sort values
data = data.sort_values(["emp_id", "mstr_dt"])

# Performance dictionary
perf_dict = {1: "Below Strong", 2: "Strong", 3: "Above Strong"}
tel = {v: k for k, vs in perf_dict.items() for v in vs}

# Map performance values
data["performance_values"] = data["performance"].map(tel)

# Helper function to calculate diff within each group
def calculate_diff(df):
    df = df.copy()
    df["perf_values_diff"] = df["performance_values"].diff(6)
    return df

# Apply the diff calculation
meta = data.head(0)
meta["perf_values_diff"] = np.float64()  # Define metadata for new column

data = data.groupby("emp_id").apply(calculate_diff, meta=meta)

# Calculate mom_performance_change
data["mom_performance_change"] = dd.map_partitions(
    lambda df: np.where(
        df["perf_values_diff"].isnull(),
        np.nan,
        np.where(df["perf_values_diff"] == 0, 0, np.where(df["perf_values_diff"] > 0, 1, -1))
    ),
    meta=(None, 'f8')
)

# Fill NaNs for the diff
data["mom_performance_change_inc"] = dd.map_partitions(
    lambda df: np.where(df["mom_performance_change"] > 0, 1, np.nan),
    meta=(None, 'f8')
)

data["mom_performance_change_dec"] = dd.map_partitions(
    lambda df: np.where(df["mom_performance_change"] < 0, 1, np.nan),
    meta=(None, 'f8')
)

# Calculate performance changes over rolling window
def calculate_rolling_sum(df, col_name):
    df[col_name] = (
        df.sort_values(["emp_id", "mstr_dt"])
        .groupby("emp_id")[col_name]
        .rolling(12, min_periods=1)
        .sum()
        .reset_index(level=0, drop=True)
    )
    return df

data = data.map_partitions(calculate_rolling_sum, col_name="mom_performance_change_inc", meta=meta)
data = data.map_partitions(calculate_rolling_sum, col_name="mom_performance_change_dec", meta=meta)

# Convert performance categories to integers and calculate cumulative sums
data["performance_category_strong_int"] = (data.performance == "Strong").astype(int)

data["performance_category_strong_cumsum"] = (
    data.groupby("emp_id")["performance_category_strong_int"]
    .cumsum()
    .fillna(0)
    .reset_index(drop=True)
)

data["performance_category_below_strong_int"] = (data.performance == "Below Strong").astype(int)

data["performance_category_below_strong_cumsum"] = (
    data.groupby("emp_id")["performance_category_below_strong_int"]
    .cumsum()
    .fillna(0)
    .reset_index(drop=True)
)

# Update performance based on month condition using Dask's map_partitions
meta = data.head(0)
data["performance"] = data.map_partitions(
    lambda df: df.assign(performance=np.where(df.month.isin([7, 1]), df["performance"], np.nan)),
    meta=meta
)

# Remember to compute the final results if needed
data = data.compute()

import dask.dataframe as dd
import numpy as np
import pandas as pd

# Example DataFrame creation for illustration purposes
data = pd.DataFrame({
    'snap_dt': pd.date_range('2023-01-01', periods=100, freq='M'),
    'emp_id': np.random.randint(1, 10, size=100),
    'performance': np.random.randint(1, 4, size=100),
    'promo_int': np.random.randint(0, 2, size=100)
})

# Convert the Pandas DataFrame to a Dask DataFrame
data = dd.from_pandas(data, npartitions=4)

# Perform renaming and type conversion
data = data.rename(columns={"snap_dt": "mstr_dt"})
data["mstr_dt"] = dd.to_datetime(data["mstr_dt"]).astype('datetime64[ns]')
data["year"] = data["mstr_dt"].dt.year
data["month"] = data["mstr_dt"].dt.month
data["performance"] = data["performance"].fillna(0)
data["promo_int"] = data["promo_int"].fillna(0)

# Remove duplicates and reset index
data = data.drop_duplicates(subset=["mstr_dt", "emp_id"]).reset_index(drop=True)

# Add sinusoidal transformation of the month
data["data_anch"] = np.sin(7 * data["performance"])

# Sort values
data = data.sort_values(["emp_id", "mstr_dt"])

# Performance dictionary
perf_dict = {1: "Below Strong", 2: "Strong", 3: "Above Strong"}
tel = {v: k for k, vs in perf_dict.items() for v in vs}

# Map performance values
data["performance_values"] = data["performance"].map(tel)

# Helper function to calculate diff within each group
def calculate_diff(df):
    df["perf_values_diff"] = df["performance_values"].diff(6)
    return df

# Apply the diff calculation
meta = data.head(0)
meta["perf_values_diff"] = np.float64()  # Define metadata for new column

data = data.groupby("emp_id").apply(calculate_diff, meta=meta)

# Calculate mom_performance_change
def calculate_mom_performance_change(df):
    return np.where(
        df["perf_values_diff"].isnull(),
        np.nan,
        np.where(df["perf_values_diff"] == 0, 0, np.where(df["perf_values_diff"] > 0, 1, -1))
    )

data["mom_performance_change"] = dd.map_partitions(
    lambda df: pd.Series(calculate_mom_performance_change(df), index=df.index),
    meta=pd.Series(dtype='float64')
)

# Fill NaNs for the diff
data["mom_performance_change_inc"] = dd.map_partitions(
    lambda df: pd.Series(np.where(df["mom_performance_change"] > 0, 1, np.nan), index=df.index),
    meta=pd.Series(dtype='float64')
)

data["mom_performance_change_dec"] = dd.map_partitions(
    lambda df: pd.Series(np.where(df["mom_performance_change"] < 0, 1, np.nan), index=df.index),
    meta=pd.Series(dtype='float64')
)

# Calculate performance changes over rolling window
def calculate_rolling_sum(df, col_name):
    df[col_name + "_rolling_sum"] = (
        df.sort_values(["emp_id", "mstr_dt"])
        .groupby("emp_id")[col_name]
        .rolling(12, min_periods=1)
        .sum()
        .reset_index(level=0, drop=True)
    )
    return df

meta = data.head(0)
meta["mom_performance_change_inc_rolling_sum"] = np.float64()
meta["mom_performance_change_dec_rolling_sum"] = np.float64()

data = data.map_partitions(calculate_rolling_sum, col_name="mom_performance_change_inc", meta=meta)
data = data.map_partitions(calculate_rolling_sum, col_name="mom_performance_change_dec", meta=meta)

# Convert performance categories to integers and calculate cumulative sums
data["performance_category_strong_int"] = (data.performance == "Strong").astype(int)

data["performance_category_strong_cumsum"] = (
    data.groupby("emp_id")["performance_category_strong_int"]
    .cumsum()
    .fillna(0)
    .reset_index(drop=True)
)

data["performance_category_below_strong_int"] = (data.performance == "Below Strong").astype(int)

data["performance_category_below_strong_cumsum"] = (
    data.groupby("emp_id")["performance_category_below_strong_int"]
    .cumsum()
    .fillna(0)
    .reset_index(drop=True)
)

# Update performance based on month condition using Dask's map_partitions
data["performance"] = data.map_partitions(
    lambda df: df.assign(performance=np.where(df.month.isin([7, 1]), df["performance"], np.nan)),
    meta=data._meta
)

# Remember to compute the final results if needed
data = data.compute()


import dask.dataframe as dd
import numpy as np
import pandas as pd
from scipy.stats import mode

# Example DataFrame creation for illustration purposes
data = pd.DataFrame({
    'snap_dt': pd.date_range('2023-01-01', periods=100, freq='M'),
    'emp_id': np.random.randint(1, 10, size=100),
    'performance': np.random.randint(1, 4, size=100),
    'promo_int': np.random.randint(0, 2, size=100)
})

# Convert the Pandas DataFrame to a Dask DataFrame
data = dd.from_pandas(data, npartitions=4)

# Perform renaming and type conversion
data = data.rename(columns={"snap_dt": "mstr_dt"})
data["mstr_dt"] = dd.to_datetime(data["mstr_dt"]).astype('datetime64[ns]')
data["year"] = data["mstr_dt"].dt.year
data["month"] = data["mstr_dt"].dt.month
data["performance"] = data["performance"].fillna(0)
data["promo_int"] = data["promo_int"].fillna(0)

# Remove duplicates and reset index
data = data.drop_duplicates(subset=["mstr_dt", "emp_id"]).reset_index(drop=True)

# Add sinusoidal transformation of the month
data["data_anch"] = np.sin(7 * data["performance"])

# Sort values
data = data.sort_values(["emp_id", "mstr_dt"])

# Performance dictionary
perf_dict = {1: "Below Strong", 2: "Strong", 3: "Above Strong"}
tel = {v: k for k, vs in perf_dict.items() for v in vs}

# Map performance values
data["performance_values"] = data["performance"].map(tel)

# Helper function to calculate diff within each group
def calculate_diff(df):
    df["perf_values_diff"] = df["performance_values"].diff(6)
    return df

# Apply the diff calculation
meta = data.head(0)
meta["perf_values_diff"] = np.float64()  # Define metadata for new column

data = data.groupby("emp_id").apply(calculate_diff, meta=meta).reset_index(drop=True)

# Calculate mom_performance_change
def calculate_mom_performance_change(df):
    return np.where(
        df["perf_values_diff"].isnull(),
        np.nan,
        np.where(df["perf_values_diff"] == 0, 0, np.where(df["perf_values_diff"] > 0, 1, -1))
    )

data["mom_performance_change"] = dd.map_partitions(
    lambda df: pd.Series(calculate_mom_performance_change(df), index=df.index),
    meta=pd.Series(dtype='float64')
)

# Fill NaNs for the diff
data["mom_performance_change_inc"] = dd.map_partitions(
    lambda df: pd.Series(np.where(df["mom_performance_change"] > 0, 1, np.nan), index=df.index),
    meta=pd.Series(dtype='float64')
)

data["mom_performance_change_dec"] = dd.map_partitions(
    lambda df: pd.Series(np.where(df["mom_performance_change"] < 0, 1, np.nan), index=df.index),
    meta=pd.Series(dtype='float64')
)

# Calculate performance changes over rolling window
def calculate_rolling_sum(df, col_name):
    df[col_name + "_rolling_sum"] = (
        df.sort_values(["emp_id", "mstr_dt"])
        .groupby("emp_id")[col_name]
        .rolling(12, min_periods=1)
        .sum()
        .reset_index(level=0, drop=True)
    )
    return df

meta = data.head(0)
meta["mom_performance_change_inc_rolling_sum"] = np.float64()
meta["mom_performance_change_dec_rolling_sum"] = np.float64()

data = data.map_partitions(calculate_rolling_sum, col_name="mom_performance_change_inc", meta=meta)
data = data.map_partitions(calculate_rolling_sum, col_name="mom_performance_change_dec", meta=meta)

# Convert performance categories to integers and calculate cumulative sums
data["performance_category_strong_int"] = (data.performance == "Strong").astype(int)

data["performance_category_strong_cumsum"] = (
    data.groupby("emp_id")["performance_category_strong_int"]
    .cumsum()
    .fillna(0)
    .reset_index(drop=True)
)

data["performance_category_below_strong_int"] = (data.performance == "Below Strong").astype(int)

data["performance_category_below_strong_cumsum"] = (
    data.groupby("emp_id")["performance_category_below_strong_int"]
    .cumsum()
    .fillna(0)
    .reset_index(drop=True)
)

# Update performance based on month condition using Dask's map_partitions
data["performance"] = data.map_partitions(
    lambda df: df.assign(performance=np.where(df.month.isin([7, 1]), df["performance"], np.nan)),
    meta=data._meta
)

# Function to calculate the rolling mode
def rolling_mode(arr, window):
    """Calculate the rolling mode of a 1D array."""
    result = np.full(arr.shape, np.nan)
    for i in range(len(arr)):
        if i >= window - 1:
            result[i] = mode(arr[i - window + 1:i + 1]).mode[0]
    return result

# Apply rolling mode function to the partitioned dataframe
def calculate_rolling_mode(df, col_name, window):
    df[col_name + "_rolling_mode"] = (
        df.sort_values(["emp_id", "mstr_dt"])
        .groupby("emp_id")[col_name]
        .transform(lambda x: rolling_mode(x.to_numpy(), window))
    )
    return df

meta = data.head(0)
meta["performance_rolling_mode"] = np.float64()

data = data.map_partitions(calculate_rolling_mode, col_name="performance", window=12, meta=meta)

# Remember to compute the final results if needed
data = data.compute()

print(data.head())

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Example data setup
data = {
    'Employee_ID': [1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3],
    'Date': ['2024-01-01', '2024-02-01', '2024-03-01', '2024-04-01', '2024-05-01', '2024-06-01'] * 3,
    'Probability': [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.10, 0.10, 0.10, 0.10, 0.10, 0.10, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03]
}

df = pd.DataFrame(data)

def simulate_employee_attrition(probabilities):
    """Simulate whether an employee attrits over 6 months based on their monthly probabilities."""
    for prob in probabilities:
        if np.random.rand() < prob:
            return 1  # Employee attrited
    return 0  # Employee did not attrit

def monte_carlo_simulation(df, num_simulations=1000):
    """Run Monte Carlo simulations for all employees."""
    results = []

    # Group by Employee_ID and collect their monthly probabilities
    grouped = df.groupby('Employee_ID')['Probability'].apply(list)

    for _ in range(num_simulations):
        total_attritions = 0
        for probabilities in grouped:
            total_attritions += simulate_employee_attrition(probabilities)
        results.append(total_attritions)

    return results

# Run the simulation
results = monte_carlo_simulation(df)

# Analyze and plot the results
plt.hist(results, bins=30, edgecolor='k', alpha=0.7)
plt.xlabel('Number of Employees Attriting At Least Once Over 6 Months')
plt.ylabel('Frequency')
plt.title('Monte Carlo Simulation of Employee Attrition')
plt.show()

# Calculate summary statistics
mean_attrition = np.mean(results)
std_attrition = np.std(results)
confidence_interval = np.percentile(results, [2.5, 97.5])

print(f"Mean number of employees attriting at least once: {mean_attrition:.2f}")
print(f"Standard deviation of attritions: {std_attrition:.2f}")
print(f"95% confidence interval: {confidence_interval}")

import pandas as pd
import numpy as np

# Example data setup
data = {
    'Employee_ID': [1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3],
    'Date': ['2024-01-01', '2024-02-01', '2024-03-01', '2024-04-01', '2024-05-01', '2024-06-01'] * 3,
    'Probability': [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.10, 0.10, 0.10, 0.10, 0.10, 0.10, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03]
}

df = pd.DataFrame(data)

def simulate_monthly_attrition(probability, num_simulations=1000):
    """Simulate whether an employee attrits in a given month across multiple simulations."""
    attrition_simulations = []
    for _ in range(num_simulations):
        attrition_simulations.append(int(np.random.rand() < probability))
    return attrition_simulations

def monte_carlo_simulation(df, num_simulations=1000):
    """Run Monte Carlo simulations and store the results for each employee and each month."""
    results = []
    
    for employee_id, group in df.groupby('Employee_ID'):
        employee_results = {'Employee_ID': employee_id}
        for index, row in group.iterrows():
            attrition_simulations = simulate_monthly_attrition(row['Probability'], num_simulations)
            employee_results[row['Date']] = attrition_simulations
        results.append(employee_results)
    
    return results

# Run the simulation
results = monte_carlo_simulation(df)

# Display the results for the first employee
print(f"Results for Employee 1:\n{results[0]}")

def simulate_employee_attrition(probabilities, num_simulations=1000):
    """Simulate whether an employee attrits each month across multiple simulations."""
    simulation_results = {month: [] for month in range(len(probabilities))}
    for _ in range(num_simulations):
        for month, prob in enumerate(probabilities):
            simulation_results[month].append(int(np.random.rand() < prob))
    return simulation_results

def monte_carlo_simulation(df, num_simulations=1000):
    """Run Monte Carlo simulations and store the results for each employee and each month."""
    results = []

    # Group by Employee_ID and collect their monthly probabilities
    grouped = df.groupby('Employee_ID')['Probability'].apply(list)

    for employee_id, probabilities in grouped.items():
        employee_results = {'Employee_ID': employee_id}
        simulation_results = simulate_employee_attrition(probabilities, num_simulations)
        for month, month_results in simulation_results.items():
            employee_results[f'Month_{month+1}'] = month_results
        results.append(employee_results)

    return results

