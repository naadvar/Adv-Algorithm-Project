I see the error - the corrupt_record is coming back as a list, not a string. Let's modify the code to handle this:
To make it show up as a properly formatted JSON, you'll need to use `from_json` to parse it first, then format it back to a readable JSON. Here's how:

```python
from pyspark.sql.functions import to_json, struct, from_json, schema_of_json, col

# First get the schema from your JSON string
sample_json = exp_3.select("experianResponse").first()[0]
json_schema = schema_of_json(sample_json)

# Parse the JSON string to a struct, then back to formatted JSON
formatted_df = exp_3.select(
    from_json("experianResponse", json_schema).alias("parsed")
).select(
    to_json(col("parsed"), Map("pretty", "true")).alias("formatted_json")
)

# Show the result
formatted_df.show(truncate=False)

# Alternatively, to see it even more clearly:
print(json.dumps(json.loads(formatted_df.first()[0]), indent=2))
```

Or if you want to work with the parsed data structure:

```python
# Parse and expand the JSON structure
parsed_df = exp_3.select(
    from_json("experianResponse", json_schema).alias("parsed")
).select("parsed.*")

# Show the expanded structure
parsed_df.show(truncate=False)
```

The key is using `from_json` to parse the JSON string first, then either expanding it or reformatting it in a readable way. Let me know if you need any adjustments to the formatting!​​​​​​​​​​​​​​​​

```python
# Get the corrupt record
corrupt_records = df.select("_corrupt_record").collect()
corrupt_record = corrupt_records[0][0]  # Get the string from the Row object

# Now clean it - make sure we're working with a string
if isinstance(corrupt_record, str):
    cleaned_json = corrupt_record.replace('\n', '').replace('\\n', '').replace('  ', ' ').strip()
    
    # Create new DataFrame from cleaned JSON
    new_df = spark.read.option("multiLine", "true")\
        .json(sc.parallelize([cleaned_json]))
else:
    print(f"Corrupt record is not a string, it's a {type(corrupt_record)}")
    # If it's a list, join the elements
    if isinstance(corrupt_record, list):
        cleaned_json = ' '.join(corrupt_record).replace('\n', '').replace('\\n', '').replace('  ', ' ').strip()
        new_df = spark.read.option("multiLine", "true")\
            .json(sc.parallelize([cleaned_json]))
```

Or alternatively, try this more robust approach:

```python
# Get all corrupt records and handle them


corrupt_records = df.select("_corrupt_record").collect()

def clean_record(record):
    if isinstance(record, list):
        return ' '.join(str(x) for x in record)
    elif isinstance(record, str):
        return record
    else:
        return str(record)

# Clean each record
cleaned_jsons = [clean_record(row[0]).replace('\n', '').replace('\\n', '').replace('  ', ' ').strip() 
                for row in corrupt_records]

# Create DataFrame 


from cleaned JSONs
new_df = spark.read.option("multiLine", "true")\
    .json(sc.parallelize(cleaned_jsons))
```

This should 
Ah, I see you're getting a JSONDecodeError with an invalid control character. This usually happens when there are hidden or special characters in the JSON string. Let's try to clean it first before parsing:

```python
# Clean the JSON string before parsing
def clean_json_string(json_str):
    # Remove any potential hidden characters and normalize whitespace
    import re
    # Remove any non-printable characters except valid whitespace
    cleaned = ''.join(char for char in json_str if char.isprintable() or char in '\n\r\t')
    # Normalize quotes (in case there are any fancy quotes)
    cleaned = cleaned.replace('"', '"').replace('"', '"')
    return cleaned


Let's try a more direct approach by just reading the raw JSON directly into a PySpark DataFrame, skipping Python's json parsing:

```python
# Create a DataFrame directly from the raw JSON string
newJson = r'''{"bureauErrorData": null,
    "bureauRawReportFeatureExecutionCode": "BureauEndStateInternalExecution",
    "bureauResponseReceivedTime": {
        "nanos": 343000000,
        "seconds": 1730323800
    },
    "checksumMetadata": {
        "checkSum": "d8f39d92f060452de22155dc209111af",
        "dataLength": "256444"
    },
    "clientRequestIdName": "applicationId",
    "clientRequestIdValue": "20000026884102",
    "dataSourceName": "ExperianConsumerCreditBureau",
    "experianCreditBureau": {
        "completionCode": "0000",
        "products": {
            "businessProfile": [],
            "checkPoint": [],
            "collectionAdvantage": [],
            "creditProfile": [],
            "customSolution": [{
                "addressInformation": [{
                    "censusGeoCode": "",
                    "city": "CENTENNIAL",
                    "dwellingType": {
                        "code": "S",
                        "value": "Single-family dwelling"
                    }
                }]
            }]
        }
    }
}'''

# Use raw string and create DataFrame directly
df = spark.read\
    .option("multiLine", "true")\
    .option("mode", "PERMISSIVE")\
    .option("allowUnquotedFieldNames", "true")\
    .option("allowSingleQuotes", "true")\
    .json(sc.parallelize([newJson]))

# Check the result
df.printSchema()
df.show(truncate=False)
```

Key changes:
1. Used a raw string (r''') to avoid any escape character issues
2. Added more permissive options to the reader
3. Bypassed Python's json parsing entirely

Let me know if this works better!​​​​​​​​​​​​​​​​
# Your original JSON string
newJson = '''{ your JSON string here }'''  # Your original JSON

# Clean and parse
cleaned_json = clean_json_string(newJson)
parsed_json = json.loads(cleaned_json)
clean_json_str = json.dumps(parsed_json)

# Create DataFrame
df = spark.read.json(sc.parallelize([clean_json_str]))
```

Or alternatively, you could try using the built-in string encode/decode to remove any problematic characters:

```python
# Alternative cleaning approach
cleaned_json = newJson.encode('ascii', 'ignore').decode('ascii')
parsed_json = json.loads(cleaned_json)
clean_json_str = json.dumps(parsed_json)

# Create DataFrame
df = spark.read.json(sc.parallelize([clean_json_str]))
```

Can you try either of these approaches and let me know if you still get the error? Also, it might help to see what characters are actually in your string at that position. You can check with:

```python
# Print the problematic area
problematic_index = 1936  # from your error message
print(f"Characters around error position: {newJson[problematic_index-5:problematic_index+5]}")
```​​​​​​​​​​​​​​​​

the case where the corrupt record is coming back as a list. Let me know if you're still seeing issues!​​​​​​​​​​​​​​​​

That's expected behavior for PySpark DataFrames when reading nested JSON - it flattens the structure using dot notation. If you want to keep the nested structure, you can keep it as a struct type or you can select specific nested fields. Here are a few ways to work with it:

```python
# 1. To see the full schema structure
df.printSchema()

# 2. To select specific nested fields
df.select(
    "bureauErrorData",
    "experianCreditBureau.completionCode",
    "experianCreditBureau.products.customSolution.addressInformation.city"
).show()

# 3. To keep nested structure, select the parent column
df.select("experianCreditBureau").show(truncate=False)

# 4. If you need the whole thing as one JSON column, you can use to_json
from pyspark.sql.functions import to_json, struct

# Convert all columns back to a single JSON string column
df_single_col = df.select(to_json(struct([df[x] for x in df.columns])).alias("json_data"))
df_single_col.show(truncate=False)
```

Which format would you prefer to work with? 
1. The flattened structure (current)
2. Keep specific nested structures
3. Everything in a single JSON column

Let me know and I can help you structure it the way you need!​​​​​​​​​​​​​​​​



Ah, you want to see the actual JSON structure. Let's try to convert it to a more readable JSON format:

```python
# First create the single column JSON
from pyspark.sql.functions import to_json, struct, col

# Convert to single JSON column and collect
json_row = df.select(to_json(struct([df[x] for x in df.columns])).alias("json_data")).collect()[0][0]

# Print the formatted JSON
import json
print(json.dumps(json.loads(json_row), indent=2))

# If you want to save this back as a DataFrame with proper JSON structure:
formatted_df = spark.createDataFrame([(json_row,)], ["json_data"])
formatted_df.show(truncate=False)
```

Or if you want to write it out:
```python
# Write the formatted JSON to a file
with open('formatted_data.json', 'w') as f:
    json.dump(json.loads(json_row), f, indent=2)
```

Let me know if you want to see the data in a different format or need help accessing specific parts of the JSON structure!​​​​​​​​​​​​​​​​



Let's try displaying it in different ways to see the actual JSON content:

```python
# Method 1: Use pandas to display
formatted_df.toPandas()

# Method 2: Force no truncation and vertical display
formatted_df.show(truncate=False, vertical=True)

# Method 3: Print the raw content
print("Raw content:")
for row in formatted_df.collect():
    print(json.dumps(json.loads(row.json_data), indent=2))

# Method 4: Register as temp view and use SQL
formatted_df.createOrReplaceTempView("json_view")
spark.sql("SELECT * FROM json_view").show(truncate=False)

# Method 5: Save and read back
# formatted_df.write.json("temp_json")
```

Could you try these different display methods and let me know which one shows the data in a way that works for you? Often the vertical=True option or the pandas conversion makes it easier to see the full content without truncation.

Also, if you just want to verify the content is correct, you could select specific fields:
```python
# Parse the JSON column back into structured fields
from pyspark.sql.functions import from_json, schema_of_json

# Get schema from your data
json_schema = schema_of_json(formatted_df.select('json_data').first()[0])

# Parse JSON with schema
parsed_df = formatted_df.select(from_json('json_data', json_schema).alias('parsed_data'))
parsed_df.select('parsed_data.*').show(truncate=False)
```​​​​​​​​​​​​​​​​
