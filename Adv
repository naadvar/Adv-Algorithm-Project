I see the error - the corrupt_record is coming back as a list, not a string. Let's modify the code to handle this:

```python
# Get the corrupt record
corrupt_records = df.select("_corrupt_record").collect()
corrupt_record = corrupt_records[0][0]  # Get the string from the Row object

# Now clean it - make sure we're working with a string
if isinstance(corrupt_record, str):
    cleaned_json = corrupt_record.replace('\n', '').replace('\\n', '').replace('  ', ' ').strip()
    
    # Create new DataFrame from cleaned JSON
    new_df = spark.read.option("multiLine", "true")\
        .json(sc.parallelize([cleaned_json]))
else:
    print(f"Corrupt record is not a string, it's a {type(corrupt_record)}")
    # If it's a list, join the elements
    if isinstance(corrupt_record, list):
        cleaned_json = ' '.join(corrupt_record).replace('\n', '').replace('\\n', '').replace('  ', ' ').strip()
        new_df = spark.read.option("multiLine", "true")\
            .json(sc.parallelize([cleaned_json]))
```

Or alternatively, try this more robust approach:

```python
# Get all corrupt records and handle them


corrupt_records = df.select("_corrupt_record").collect()

def clean_record(record):
    if isinstance(record, list):
        return ' '.join(str(x) for x in record)
    elif isinstance(record, str):
        return record
    else:
        return str(record)

# Clean each record
cleaned_jsons = [clean_record(row[0]).replace('\n', '').replace('\\n', '').replace('  ', ' ').strip() 
                for row in corrupt_records]

# Create DataFrame 


from cleaned JSONs
new_df = spark.read.option("multiLine", "true")\
    .json(sc.parallelize(cleaned_jsons))
```

This should 
Ah, I see you're getting a JSONDecodeError with an invalid control character. This usually happens when there are hidden or special characters in the JSON string. Let's try to clean it first before parsing:

```python
# Clean the JSON string before parsing
def clean_json_string(json_str):
    # Remove any potential hidden characters and normalize whitespace
    import re
    # Remove any non-printable characters except valid whitespace
    cleaned = ''.join(char for char in json_str if char.isprintable() or char in '\n\r\t')
    # Normalize quotes (in case there are any fancy quotes)
    cleaned = cleaned.replace('"', '"').replace('"', '"')
    return cleaned


Let's try a more direct approach by just reading the raw JSON directly into a PySpark DataFrame, skipping Python's json parsing:

```python
# Create a DataFrame directly from the raw JSON string
newJson = r'''{"bureauErrorData": null,
    "bureauRawReportFeatureExecutionCode": "BureauEndStateInternalExecution",
    "bureauResponseReceivedTime": {
        "nanos": 343000000,
        "seconds": 1730323800
    },
    "checksumMetadata": {
        "checkSum": "d8f39d92f060452de22155dc209111af",
        "dataLength": "256444"
    },
    "clientRequestIdName": "applicationId",
    "clientRequestIdValue": "20000026884102",
    "dataSourceName": "ExperianConsumerCreditBureau",
    "experianCreditBureau": {
        "completionCode": "0000",
        "products": {
            "businessProfile": [],
            "checkPoint": [],
            "collectionAdvantage": [],
            "creditProfile": [],
            "customSolution": [{
                "addressInformation": [{
                    "censusGeoCode": "",
                    "city": "CENTENNIAL",
                    "dwellingType": {
                        "code": "S",
                        "value": "Single-family dwelling"
                    }
                }]
            }]
        }
    }
}'''

# Use raw string and create DataFrame directly
df = spark.read\
    .option("multiLine", "true")\
    .option("mode", "PERMISSIVE")\
    .option("allowUnquotedFieldNames", "true")\
    .option("allowSingleQuotes", "true")\
    .json(sc.parallelize([newJson]))

# Check the result
df.printSchema()
df.show(truncate=False)
```

Key changes:
1. Used a raw string (r''') to avoid any escape character issues
2. Added more permissive options to the reader
3. Bypassed Python's json parsing entirely

Let me know if this works better!​​​​​​​​​​​​​​​​
# Your original JSON string
newJson = '''{ your JSON string here }'''  # Your original JSON

# Clean and parse
cleaned_json = clean_json_string(newJson)
parsed_json = json.loads(cleaned_json)
clean_json_str = json.dumps(parsed_json)

# Create DataFrame
df = spark.read.json(sc.parallelize([clean_json_str]))
```

Or alternatively, you could try using the built-in string encode/decode to remove any problematic characters:

```python
# Alternative cleaning approach
cleaned_json = newJson.encode('ascii', 'ignore').decode('ascii')
parsed_json = json.loads(cleaned_json)
clean_json_str = json.dumps(parsed_json)

# Create DataFrame
df = spark.read.json(sc.parallelize([clean_json_str]))
```

Can you try either of these approaches and let me know if you still get the error? Also, it might help to see what characters are actually in your string at that position. You can check with:

```python
# Print the problematic area
problematic_index = 1936  # from your error message
print(f"Characters around error position: {newJson[problematic_index-5:problematic_index+5]}")
```​​​​​​​​​​​​​​​​

the case where the corrupt record is coming back as a list. Let me know if you're still seeing issues!​​​​​​​​​​​​​​​​

That's expected behavior for PySpark DataFrames when reading nested JSON - it flattens the structure using dot notation. If you want to keep the nested structure, you can keep it as a struct type or you can select specific nested fields. Here are a few ways to work with it:

```python
# 1. To see the full schema structure
df.printSchema()

# 2. To select specific nested fields
df.select(
    "bureauErrorData",
    "experianCreditBureau.completionCode",
    "experianCreditBureau.products.customSolution.addressInformation.city"
).show()

# 3. To keep nested structure, select the parent column
df.select("experianCreditBureau").show(truncate=False)

# 4. If you need the whole thing as one JSON column, you can use to_json
from pyspark.sql.functions import to_json, struct

# Convert all columns back to a single JSON string column
df_single_col = df.select(to_json(struct([df[x] for x in df.columns])).alias("json_data"))
df_single_col.show(truncate=False)
```

Which format would you prefer to work with? 
1. The flattened structure (current)
2. Keep specific nested structures
3. Everything in a single JSON column

Let me know and I can help you structure it the way you need!​​​​​​​​​​​​​​​​


