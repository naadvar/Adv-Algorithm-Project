import re
from pyspark.sql import functions as F
from pyspark.sql.types import StructType, StructField, StringType, ArrayType, MapType

# Define a recursive function to clean up empty values and track removed keys
def clean_and_flatten(data, parent_key=""):
    removed_keys = []
    flattened_data = {}
    whitespace_pattern = re.compile(r'^\s+$')  # Regex pattern to match strings with only whitespace

    if isinstance(data, dict):
        for k, v in data.items():
            full_key = f"{parent_key}.{k}" if parent_key else k
            if v is None or (isinstance(v, str) and whitespace_pattern.match(v)):
                removed_keys.append(full_key)  # Track the key if it's removed
            else:
                # Recursively flatten nested dictionaries
                nested_flattened_data, nested_removed_keys = clean_and_flatten(v, full_key)
                flattened_data.update(nested_flattened_data)
                removed_keys.extend(nested_removed_keys)

    elif isinstance(data, list):
        for i, item in enumerate(data):
            full_key = f"{parent_key}[{i}]"
            if item is None or (isinstance(item, str) and whitespace_pattern.match(item)):
                removed_keys.append(full_key)  # Track the index if an item is removed
            else:
                nested_flattened_data, nested_removed_keys = clean_and_flatten(item, full_key)
                flattened_data.update(nested_flattened_data)
                removed_keys.extend(nested_removed_keys)

    else:
        # If it's a primitive type, add it directly unless it's empty/invalid
        if data is None or (isinstance(data, str) and whitespace_pattern.match(data)):
            removed_keys.append(parent_key)  # Track the key if it's removed
        else:
            flattened_data[parent_key] = data

    return flattened_data, removed_keys

# Define a helper UDF function to clean and flatten the nested structure
def udf_clean_and_flatten(data):
    flattened_data, removed_keys = clean_and_flatten(data)
    return (flattened_data, removed_keys)

# Define the schema for the returned struct from the UDF
schema = StructType([
    StructField("cleaned_data", MapType(StringType(), StringType())),  # Use MapType to store flattened structure
    StructField("removed_keys", ArrayType(StringType()))
])

# Register the UDF with the defined schema
clean_and_flatten_udf = F.udf(udf_clean_and_flatten, schema)

# Apply the UDF to the entire experianCreditBureau structure
exp_1 = exp_1.withColumn(
    'cleaned_experianCreditBureau',
    clean_and_flatten_udf(F.col('experianResponse.experianCreditBureau'))
)

# Extract flattened cleaned data and removed keys as separate columns
exp_1 = exp_1.withColumn("flattened_data", F.col("cleaned_experianCreditBureau.cleaned_data")) \
             .withColumn("removed_keys", F.col("cleaned_experianCreditBureau.removed_keys")) \
             .drop("cleaned_experianCreditBureau")
